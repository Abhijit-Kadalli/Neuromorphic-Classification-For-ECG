{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1BlTqunB73-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f0a8e12-7e61-4a12-f80c-51dcaf905d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.4/411.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install snntorch brevitas --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iqXUHT7pEGDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f61ca860-a6af-4262-e26a-9366ab3c747c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import pywt\n",
        "from scipy import stats\n",
        "\n",
        "# from utils import denoise\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (30,6)\n",
        "plt.rcParams['lines.linewidth'] = 1\n",
        "plt.rcParams['lines.color'] = 'b'\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(self):\n",
        "        self.dataset = None\n",
        "        self.data_path = None\n",
        "\n",
        "\n",
        "    def denoise(data):\n",
        "        w = pywt.Wavelet('sym4')\n",
        "        maxlev = pywt.dwt_max_level(len(data), w.dec_len)\n",
        "        threshold = 0.04 # Threshold for filtering\n",
        "\n",
        "        coeffs = pywt.wavedec(data, 'sym4', level=maxlev)\n",
        "        for i in range(1, len(coeffs)):\n",
        "            coeffs[i] = pywt.threshold(coeffs[i], threshold*max(coeffs[i]))\n",
        "\n",
        "        datarec = pywt.waverec(coeffs, 'sym4')\n",
        "\n",
        "        return datarec\n",
        "\n",
        "    def load_dataset(self, data_path):\n",
        "\n",
        "        print(\"Loading dataset from: \", data_path)\n",
        "\n",
        "        records = list()\n",
        "        annotations = list()\n",
        "\n",
        "        for file in os.listdir(data_path):\n",
        "            if file.endswith(\".csv\"):\n",
        "                records.append(data_path+file)\n",
        "            elif file.endswith(\".txt\"):\n",
        "                annotations.append(data_path+file)\n",
        "\n",
        "        records.sort()\n",
        "        annotations.sort()\n",
        "\n",
        "        # print(records[0], annotations[0])\n",
        "\n",
        "        return records, annotations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def preprocess(self,records,annotations):\n",
        "        maximum_counting = 10000\n",
        "        window_size = 180\n",
        "        classes = ['N', 'L', 'R', 'A', 'V']\n",
        "        n_classes = 5\n",
        "        count_classes = [0]*5\n",
        "        X = list()\n",
        "        y = list()\n",
        "        for r in range(0,len(records)):\n",
        "            signals = []\n",
        "\n",
        "            with open(records[r], 'rt') as csvfile:\n",
        "                spamreader = csv.reader(csvfile, delimiter=',', quotechar='|') # read CSV file\\\n",
        "                row_index = -1\n",
        "                for row in spamreader:\n",
        "                    if(row_index >= 0):\n",
        "                        signals.insert(row_index, int(row[1]))\n",
        "                    row_index += 1\n",
        "\n",
        "            # # Plot an example to the signals\n",
        "            # if r == 6:\n",
        "            #     # Plot each patient's signal\n",
        "            #     plt.title(records[6] + \" Wave\")\n",
        "            #     plt.plot(signals[0:700])\n",
        "            #     plt.show()\n",
        "            def denoise(data):\n",
        "                w = pywt.Wavelet('sym4')\n",
        "                maxlev = pywt.dwt_max_level(len(data), w.dec_len)\n",
        "                threshold = 0.04 # Threshold for filtering\n",
        "\n",
        "                coeffs = pywt.wavedec(data, 'sym4', level=maxlev)\n",
        "                for i in range(1, len(coeffs)):\n",
        "                    coeffs[i] = pywt.threshold(coeffs[i], threshold*max(coeffs[i]))\n",
        "\n",
        "                datarec = pywt.waverec(coeffs, 'sym4')\n",
        "\n",
        "                return datarec\n",
        "            signals = denoise(signals)\n",
        "            # # Plot an example to the signals\n",
        "            # if r == 6:\n",
        "            #     # Plot each patient's signal\n",
        "            #     plt.title(records[6] + \" wave after denoised\")\n",
        "            #     plt.plot(signals[0:700])\n",
        "            #     plt.show()\n",
        "\n",
        "            # signals = stats.zscore(signals)\n",
        "            # # Plot an example to the signals\n",
        "            # if r == 6:\n",
        "            #     # Plot each patient's signal\n",
        "            #     plt.title(records[6] + \" wave after z-score normalization \")\n",
        "            #     plt.plot(signals[0:700])\n",
        "            #     plt.show()\n",
        "\n",
        "            # Read anotations: R position and Arrhythmia class\n",
        "            example_beat_printed = False\n",
        "            with open(annotations[r], 'r') as fileID:\n",
        "                data = fileID.readlines()\n",
        "                beat = list()\n",
        "\n",
        "                for d in range(1, len(data)): # 0 index is Chart Head\n",
        "                        splitted = data[d].split(' ')\n",
        "                        splitted = filter(None, splitted)\n",
        "                        next(splitted) # Time... Clipping\n",
        "                        pos = int(next(splitted)) # Sample ID\n",
        "                        arrhythmia_type = next(splitted) # Type\n",
        "                        if(arrhythmia_type in classes):\n",
        "                            arrhythmia_index = classes.index(arrhythmia_type)\n",
        "                    # if count_classes[arrhythmia_index] > maximum_counting: # avoid overfitting\n",
        "                            #    pass\n",
        "                        #else:\n",
        "                            count_classes[arrhythmia_index] += 1\n",
        "                            if(window_size <= pos and pos < (len(signals) - window_size)):\n",
        "                                beat = signals[pos-window_size:pos+window_size]     ## REPLACE WITH R-PEAK DETECTION\n",
        "                                # Plot an example to a beat\n",
        "                                # if r == 6 and not example_beat_printed:\n",
        "                                #     plt.title(\"A Beat from \" + records[6] + \" Wave\")\n",
        "                                #     plt.plot(beat)\n",
        "                                #     plt.show()\n",
        "                                #     example_beat_printed = True\n",
        "\n",
        "                                X.append(beat)\n",
        "                                y.append(arrhythmia_index)\n",
        "\n",
        "        return X, y"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HfnCdLgEqgZc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "import pywt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from dataset import Dataset\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (30,6)\n",
        "plt.rcParams['lines.linewidth'] = 1\n",
        "plt.rcParams['lines.color'] = 'b'\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "\n",
        "def plot_signals(signals, title):\n",
        "\n",
        "    plt.title(\"ECG signal: \" + title)\n",
        "    plt.plot(signals)\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BS2gZn2Zqigi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset()\n",
        "data_path = '/content/drive/MyDrive/dataset/'\n",
        "\n",
        "records, annotations = dataset.load_dataset(data_path)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "X, y = dataset.preprocess(records, annotations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxQKIGGDq0YU",
        "outputId": "bee5aadf-6d09-49c9-efb3-1cd5f21d08b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from:  /content/drive/MyDrive/dataset/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, y,transform=None):\n",
        "        self.data = torch.tensor(X, dtype=torch.float32)\n",
        "        self.targets = torch.tensor(y, dtype=torch.long)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]\n"
      ],
      "metadata": {
        "id": "3hMuC4WArfXy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "min_value = np.min(X)\n",
        "max_value = np.max(X)\n",
        "\n",
        "normalised_X = (X - min_value) / (max_value - min_value)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(normalised_X, y, random_state=104, train_size=0.8, shuffle=True)"
      ],
      "metadata": {
        "id": "srsWcalOr0P7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "test_dataset = CustomDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "_E338urCriTh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Assuming you have loaded your data and labels into train_loader and test_loader\n",
        "\n",
        "class ECGNet(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(ECGNet, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5)\n",
        "\n",
        "        # Calculate the output size after convolutions and pooling\n",
        "        self.conv_out_size = self.calculate_conv_output_size()\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * self.conv_out_size, 100)\n",
        "        self.fc2 = nn.Linear(100, num_classes)\n",
        "\n",
        "    def calculate_conv_output_size(self):\n",
        "        # Calculate the output size after convolutions and pooling\n",
        "        x = torch.randn(1, 1, 360)  # Example input\n",
        "        x = self.pool(self.conv1(x))\n",
        "        x = self.pool(self.conv2(x))\n",
        "        return x.size(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.pool(self.conv1(x)))\n",
        "        x = torch.relu(self.pool(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output for fully connected layers\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = ECGNet(num_classes=5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        outputs = model(inputs.unsqueeze(1))  # Add 1 channel dimension for the input\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs.unsqueeze(1))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy on the test data: {}%'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Bm_MEdGRUKh",
        "outputId": "14dfbccb-1f54-45c9-977a-be9a4db2772f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/626], Loss: 0.9042\n",
            "Epoch [1/10], Step [200/626], Loss: 0.9640\n",
            "Epoch [1/10], Step [300/626], Loss: 0.5959\n",
            "Epoch [1/10], Step [400/626], Loss: 0.4928\n",
            "Epoch [1/10], Step [500/626], Loss: 0.3924\n",
            "Epoch [1/10], Step [600/626], Loss: 0.4230\n",
            "Epoch [2/10], Step [100/626], Loss: 0.2658\n",
            "Epoch [2/10], Step [200/626], Loss: 0.2076\n",
            "Epoch [2/10], Step [300/626], Loss: 0.3716\n",
            "Epoch [2/10], Step [400/626], Loss: 0.4192\n",
            "Epoch [2/10], Step [500/626], Loss: 0.2190\n",
            "Epoch [2/10], Step [600/626], Loss: 0.3198\n",
            "Epoch [3/10], Step [100/626], Loss: 0.1128\n",
            "Epoch [3/10], Step [200/626], Loss: 0.1747\n",
            "Epoch [3/10], Step [300/626], Loss: 0.1622\n",
            "Epoch [3/10], Step [400/626], Loss: 0.1542\n",
            "Epoch [3/10], Step [500/626], Loss: 0.1449\n",
            "Epoch [3/10], Step [600/626], Loss: 0.2160\n",
            "Epoch [4/10], Step [100/626], Loss: 0.0714\n",
            "Epoch [4/10], Step [200/626], Loss: 0.1472\n",
            "Epoch [4/10], Step [300/626], Loss: 0.1206\n",
            "Epoch [4/10], Step [400/626], Loss: 0.1134\n",
            "Epoch [4/10], Step [500/626], Loss: 0.1547\n",
            "Epoch [4/10], Step [600/626], Loss: 0.0430\n",
            "Epoch [5/10], Step [100/626], Loss: 0.2045\n",
            "Epoch [5/10], Step [200/626], Loss: 0.0716\n",
            "Epoch [5/10], Step [300/626], Loss: 0.0962\n",
            "Epoch [5/10], Step [400/626], Loss: 0.0826\n",
            "Epoch [5/10], Step [500/626], Loss: 0.1581\n",
            "Epoch [5/10], Step [600/626], Loss: 0.0877\n",
            "Epoch [6/10], Step [100/626], Loss: 0.1513\n",
            "Epoch [6/10], Step [200/626], Loss: 0.1370\n",
            "Epoch [6/10], Step [300/626], Loss: 0.0841\n",
            "Epoch [6/10], Step [400/626], Loss: 0.0904\n",
            "Epoch [6/10], Step [500/626], Loss: 0.0644\n",
            "Epoch [6/10], Step [600/626], Loss: 0.1001\n",
            "Epoch [7/10], Step [100/626], Loss: 0.0422\n",
            "Epoch [7/10], Step [200/626], Loss: 0.0460\n",
            "Epoch [7/10], Step [300/626], Loss: 0.1978\n",
            "Epoch [7/10], Step [400/626], Loss: 0.1155\n",
            "Epoch [7/10], Step [500/626], Loss: 0.0622\n",
            "Epoch [7/10], Step [600/626], Loss: 0.0954\n",
            "Epoch [8/10], Step [100/626], Loss: 0.0613\n",
            "Epoch [8/10], Step [200/626], Loss: 0.0396\n",
            "Epoch [8/10], Step [300/626], Loss: 0.0349\n",
            "Epoch [8/10], Step [400/626], Loss: 0.0203\n",
            "Epoch [8/10], Step [500/626], Loss: 0.0649\n",
            "Epoch [8/10], Step [600/626], Loss: 0.0711\n",
            "Epoch [9/10], Step [100/626], Loss: 0.0311\n",
            "Epoch [9/10], Step [200/626], Loss: 0.0466\n",
            "Epoch [9/10], Step [300/626], Loss: 0.0736\n",
            "Epoch [9/10], Step [400/626], Loss: 0.0867\n",
            "Epoch [9/10], Step [500/626], Loss: 0.1185\n",
            "Epoch [9/10], Step [600/626], Loss: 0.0377\n",
            "Epoch [10/10], Step [100/626], Loss: 0.0219\n",
            "Epoch [10/10], Step [200/626], Loss: 0.1188\n",
            "Epoch [10/10], Step [300/626], Loss: 0.0891\n",
            "Epoch [10/10], Step [400/626], Loss: 0.0442\n",
            "Epoch [10/10], Step [500/626], Loss: 0.0858\n",
            "Epoch [10/10], Step [600/626], Loss: 0.0900\n",
            "Accuracy on the test data: 98.28525721141828%\n"
          ]
        }
      ]
    }
  ]
}